{"cells":[{"cell_type":"markdown","metadata":{"id":"qhqVSofWCeHl"},"source":["# Assignment 2 #\n","### Due: Friday, September 22 to be submitted via Canvas by 11:59 pm ###\n","### Total points: **90** ###"]},{"cell_type":"markdown","metadata":{"id":"_oWED2hlDCVs"},"source":["Your homework should be written in a python notebook. If you prefer, you can work in groups of two. For any question that requires a handwritten solution, you may upload scanned images of your solution in the notebook or attach them to the assignment . You may write your solution using markdown as well.\n","\n","### Note that:\n","###1. Only one student per group needs to submit the assignment on Canvas;\n","###2. Make sure to include both students' names and UT EIDs in your submitted notebook;\n","###3. Please make sure your code runs, the graphs and figures are displayed in your notebook before submitting. (%matplotlib inline)\n","###4. Late submissions receive 0 points.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BElYjEVrDW_C"},"source":["# Question 1: Bias and Variance (**10 pts**)\n","\n","(a). (**4 pts**) Describe the difference between model bias and the bias of a point estimator.\n","> While a model uses input data to predict a value as its output, a point estimator is a statistic representing an unknown population parameter. Note that a point estimator can be mean, variance, and even a regression coefficient. Now, model bias refers to the inaccuracy in the predictive model's ouput. It represents how far the model's prediction was from the true value of a given input. Meanwhile, the bias of a point estimator is the error between the point estimator itself (the predicted parameter value) and the true value of the parameter. It measures whether the estimator is an over or underestimate of the parameter of interest. Hence, while the model bias is a reflection of the accuracy of entire predictive model and how well it approximates the underlying data generation, the bias of a point estimator corresponds to a one specific statistical measure and describes how closely the point estimator approximates the true parameter value.\n","\n","(b) (**6 pts**). How can you use a learning curve to determine whether a model is overfitting  (for a given sample size)? Discuss this with respect to the observed train and validation error curves. How does your answer change if the model you are trying to determine is underfitting?\n"]},{"cell_type":"markdown","metadata":{"id":"SODU_RZZ56qR"},"source":["# Question 2: Bias-Variance Exploration (**20 points**)\n","(Notation: y, y-hat, not t, y)\n","\n","Generate a set of data points $(X,Y)$ where both X and Y are 1-dimensional with 512 data points. The $i^{th}$ point of the vector X, $x_i = (i)/n$ for each $i$ in $[1,512]$. Then, for each of the $x_i$'s, obtain $y_i = 2sin(\\pi\n","x_i)cos(3\\pi x_i^2).$ Also obtain another pair of this dataset where you keep the X same but obtain Y_noisy by adding a Gaussian noise to the previously obtained Y, i.e., $y_i = 2sin(\\pi x_i)cos(3\\pi x_i^2) + N(0,0.5)$.\n","\n","a) (2 points) Plot the above obtained datasets (X, Y) and (X, Y_noisy).\n","\n","b) (5 points) Consider an estimator of Y, f(x) that divides the x's into $K$ bins such that there are $m = n/K$ data points in each bin and the predicted y for all points in that bin is the mean of all the $y$'s in the bin.\n","$$\n","f(x) = \\sum_{j=1}^{K} \\bar{y}_j \\mathbb{1}(x \\in \\text{bin } j)\n","$$\n","where\n","$$\n","\\bar{y}_j = \\frac{1}{m}\\sum_{i=j*m}^{(j+1)*m-1} y_i.\n","$$\n","Implements this estimator.\n","\n","c) (3 points) What do you think will happen to bias and variance when the number of bins $K$ is kept small and when $K$ is kept large.\n","\n","d) (5 points) Now vary the number of bins $K \\in \\{1,4,8,16,32,64,128,256\\}$ and obtain the values of mean squared error, bias and variance for this estimator assuming target is Y. Plot these 3 curves on a single figure.\n","\n","e) (5 points) Repeat (d) but now consider Y_noisy as the target variable."]},{"cell_type":"markdown","metadata":{"id":"8osSuMrKNkbV"},"source":["# Question 3: Stochastic Gradient Descent Improvements (**10 pts**)\n","\n","## Part 1. (**5 pts**) ##\n","Read this [blog](https://medium.com/optimization-algorithms-for-deep-neural-networks/gradient-descent-with-momentum-dce805cd8de8) on medium and describe in your own words how momentum leads to a faster convergence of the loss function.\n"]},{"cell_type":"markdown","metadata":{"id":"ymgkxoqCrbDr"},"source":["## Part 2. (**5 pts**) ##\n","Read this [blog](https://sweta-nit.medium.com/batch-mini-batch-and-stochastic-gradient-descent-e9bc4cacd461) on medium and explain in your own words the advantages of Mini-batch Stochastic Gradient Descent.\n"]},{"cell_type":"markdown","metadata":{"id":"Au9dqqwEERjQ"},"source":["# Question 4: Stochastic Gradient Descent (30 pts)\n","\n","## Part 1. (**10 pts**) Stochastic gradient descent derivation ##\n","\n","Use stochastic gradient descent to derive the coefficent updates (assuming squared loss is being used as the cost function) for the 4 coefficients $w_0, w_1, w_2, w_3$ in this model：\n","\n","$$ y = w_0 + w_1e^{-x_1} + w_2 x_1 + w_3x_1x_2 $$\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aajnSTB-rbDr"},"source":["## Part 2. (**20 pts**) Stochastic gradient descent coding ##\n","\n","Code an SGD solution in Python for this non-linear model$$ y = w_0 + w_1e^{-x_1} + w_2x_1 + w_3x_1x_2 $$  The template of the solution class is given. The `init` method of the class takes as input the learning rate, regularization constant and number of epochs. The `fit` method must take as input `X`, `y`. The `predict` method takes an X value (optionally, an array of values).\n","\n","a) (**15 pts**) Use the expression derived in Part 1 to predict the data given in 'SGD_samples.csv', for 15 epochs, using learning rates: [0, .0001, .001, .01, 0.1, 1, 10, 100] and regularization (ridge regression) constants: [0,10,100]. For the best 2 combinations of learning_rate and regularization for SGD, plot MSE and the $w$ parameters as a function of epoch (for 15 epochs) .\n","\n","b) (**5 pts**) Report the MSE of the two best combinations of learning rate and regularization constant."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hHHQcoWLY0f"},"outputs":[],"source":["# Only use this code block if you are using Google Colab.\n","# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n","from google.colab import files\n","\n","## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file.\n","## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n","uploaded = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6TbikpkLY20"},"outputs":[],"source":["%matplotlib inline\n","import random\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","class Regression:\n","\n","    def __init__(self, learning_rate, regularization, n_epoch):\n","        self.learning_rate = learning_rate\n","        self.n_epoch = n_epoch\n","        self.regularization = regularization\n","        # initialize whichever variables you would need here\n","        self.coef = np.zeros(4)\n","\n","    def sgd(self, gradient):\n","        # Update the self.coef using SGD\n","        ### START CODE ###\n","        self.coef = None\n","        ### END CODE ###\n","\n","    def fit(self, X, y, update_rule='sgd', plot=False):\n","        mse = []\n","        coefs = []\n","        X = self.get_features(X)\n","        for epoch in range(self.n_epoch):\n","            for i in range(X.shape[0]):\n","                # Compute error\n","                ### START CODE ###\n","                ### END CODE ###\n","\n","                # Compute gradients\n","                ### START CODE ###\n","                ### END CODE ###\n","\n","                # Update weights\n","                ### START CODE ###\n","                ### END CODE ###\n","\n","            coefs.append(self.coef)\n","            residuals = y - self.linearPredict(X)\n","            mse.append(np.mean(residuals**2))\n","\n","        self.lowest_mse = mse[-1]\n","        if plot == True:\n","            plt.figure()\n","            plt.plot(range(self.n_epoch),mse)\n","            plt.xlabel('epoch')\n","            plt.ylabel('MSE')\n","            plt.figure()\n","            coefs = np.array(coefs)\n","            plt.plot(range(self.n_epoch),coefs[:,0],label='w0')\n","            plt.plot(range(self.n_epoch),coefs[:,1],label='w1')\n","            plt.plot(range(self.n_epoch),coefs[:,2],label='w2')\n","            plt.plot(range(self.n_epoch),coefs[:,3],label='w3')\n","            plt.legend()\n","            plt.xlabel('epoch')\n","            plt.ylabel('parameter value')\n","\n","    def get_features(self, X):\n","        '''\n","        this output of this function can be used to compute the gradient in `fit`\n","        '''\n","        x = np.zeros((X.shape[0], 4))\n","        x[:,0] = 1\n","        x[:,1] = np.exp(-X[:,0])\n","        x[:,2] = X[:,0]\n","        x[:,3] = X[:,0]*X[:,1]\n","\n","        return x\n","\n","    def linearPredict(self, X):\n","        # Compute the dot product of self.coef and X\n","        ### START CODE ###\n","        return None\n","        ### END CODE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nji0zmtkLY5B"},"outputs":[],"source":["data = pd.read_csv('SGD_samples.csv')\n","X = np.array([data['x1'].values, data['x2'].values]).T\n","y = data['y'].values\n","n_epochs = 15\n","learning_rate = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n","regularization = [0, 10, 100]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKOQ-MnirbDs"},"outputs":[],"source":["# Iterate through all combinations of learning rates and regularization strength\n","# Use your Regression class to fit the data and record MSEs\n","### START CODE ###\n","### END CODE ###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aep9Gw-KrbDs"},"outputs":[],"source":["# For the best two combinations, use the plot option in Regression.fit() to plot MSE and parameters as a function of epoch (15 epochs)\n","### START CODE ###\n","### END CODE ###"]},{"cell_type":"markdown","metadata":{"id":"lNFzAIHxrbDs"},"source":["# Question 5: Visualizing Gradient Descent (**20 pts**) #"]},{"cell_type":"markdown","metadata":{"id":"7lIXK948rbDs"},"source":["## Part 1. **(10 pts)** Coding ##\n","\n","In this exercise, you are going to visualize four batch update steps of gradient descent for a  linear regression model with two parameters (i.e. weights, indicated by $\\theta$).\n","\n","The true target function is $t = \\theta_{0} + \\theta_{1}x$ with $\\theta_{0}=2$ and $\\theta_{1}=0.5$.\n","\n","Try the following two initializations:\n","* $\\theta_{0}=0$ and $\\theta_{1}=0$\n","* $\\theta_{0}=0$ and $\\theta_{1}=-4$\n","\n","and try the following three learning rates:\n","* 0.5\n","* 1\n","* 2.1\n","\n","Therefore, there will be **six** combinations or settings to consider in total. For each setting, you will plot (a) the data and the changing linear regression fit and (b) the model parameters moving in the weight space after every update."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l56rKW9hrbDs"},"outputs":[],"source":["# Generate data\n","np.random.seed(42)\n","m = 20\n","theta0_true = 2\n","theta1_true = 0.5\n","x = np.linspace(-1,1,m)\n","y = theta0_true + theta1_true * x + np.random.normal(0, 0.2, size=x.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0H_MsMVCrbDs"},"outputs":[],"source":["def loss_func(theta0, theta1):\n","    theta0 = np.atleast_3d(np.asarray(theta0))\n","    theta1 = np.atleast_3d(np.asarray(theta1))\n","    return np.average((y - model(x, theta0, theta1))**2, axis=2)/2\n","\n","def model(x, theta0, theta1):\n","    return theta0 + theta1 * x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NTks0r_TrbDs"},"outputs":[],"source":["# Experiment with different initialization and learning rate combinations\n","### START CODE ###\n","init_list = [None]\n","lr_list = [None]\n","### END CODE ###\n","\n","# Left column shows the data and the changing linear regression models\n","# Right column shows the model parameters moving over the loss landscape\n","fig, ax = plt.subplots(nrows=len(init_list) * len(lr_list), ncols=2, figsize=(12, 36))\n","\n","for i, init in enumerate(init_list):\n","    for j, lr in enumerate(lr_list):\n","        row_idx = i * len(lr_list) + j\n","        ax[row_idx][0].scatter(x, y, marker='x', s=40, color='k')\n","        theta0_grid = np.linspace(-1,5,101)\n","        theta1_grid = np.linspace(-5,5,101)\n","        L_grid = loss_func(theta0_grid[np.newaxis,:,np.newaxis],\n","                           theta1_grid[:,np.newaxis,np.newaxis])\n","\n","        # A labeled contour plot for the right column\n","        X, Y = np.meshgrid(theta0_grid, theta1_grid)\n","        contours = ax[row_idx][1].contour(X, Y, L_grid, 30)\n","        ax[row_idx][1].clabel(contours)\n","        # The target parameter values indicated on the loss function contour plot\n","        ax[row_idx][1].scatter([theta0_true]*2,[theta1_true]*2,s=[50,10], color=['k','w'])\n","\n","        # Take N = 4 steps with learning rate alpha down the steepest gradient, starting at init\n","        N = 4\n","        theta = [init] # placeholder list for storing historical parameters\n","        L = [loss_func(*theta[0])[0]] # placeholder list for storing historical loss values\n","        for _ in range(N):\n","            last_theta = theta[-1]\n","            this_theta = np.empty((2,))\n","            # Update theta\n","            ### START CODE ### (2 lines of code)\n","            this_theta[0] = None\n","            this_theta[1] = None\n","            ### END CODE ###\n","            theta.append(this_theta)\n","            L.append(loss_func(*this_theta))\n","\n","        # Annotate the loss function plot with coloured points indicating the\n","        # parameters chosen and red arrows indicating the steps down the gradient.\n","        # Also plot the fit function on the LHS data plot in a matching colour.\n","        colors = ['b', 'g', 'm', 'c', 'orange']\n","        ax[row_idx][0].plot(x, model(x, *theta[0]), color=colors[0], lw=2,\n","                   label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[0]))\n","        for k in range(1,N+1):\n","            ax[row_idx][1].annotate('', xy=theta[k], xytext=theta[k-1],\n","                           arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},\n","                           va='center', ha='center')\n","            ax[row_idx][0].plot(x, model(x, *theta[k]), color=colors[k], lw=2,\n","                   label=r'$\\theta_0 = {:.3f}, \\theta_1 = {:.3f}$'.format(*theta[k]))\n","        ax[row_idx][1].scatter(*zip(*theta), c=colors, s=40, lw=0)\n","\n","        # Labels and titles.\n","        ax[row_idx][1].set_xlabel(r'$\\theta_0$')\n","        ax[row_idx][1].set_ylabel(r'$\\theta_1$')\n","        ax[row_idx][1].set_title(f'Loss function (Init:[{init[0]},{init[1]}], LR:{lr})')\n","        ax[row_idx][0].set_xlabel(r'$x$')\n","        ax[row_idx][0].set_ylabel(r'$y$')\n","        ax[row_idx][0].set_title(f'Data and Fit (Init:[{init[0]},{init[1]}], LR:{lr})')\n","        axbox = ax[row_idx][0].get_position()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"hOErdM90rbDs"},"source":["## Part 2. **(5 pts)** ##\n","For the experiment above, briefly summarize what you observed about the impact of (i) initialization and (ii) learning rate, on the evolution of the model parameters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aW2LJ44vsHSo"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}
